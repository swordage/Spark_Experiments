# 实验目的
学习Spark Streaming编程模型，熟悉Spark Streaming相关API。

# 实验原理
我们基于Spark Streaming简单实现一个分析系统，使之包括以下分析功能。

- **流量分析**。一段时间内用户网站的流量变化趋势，针对不同的 IP 对用户网站的流量进行细分。常见指标是总 PV 和各 IP 的PV。
- **来源分析**。各种搜索引擎来源给用户网站带来的流量情况，需要精确到具体搜索引擎、具体关键词。通过来源分析，用户可以及时了解哪种类型的来源为其带来了更多访客。常见指标是搜索引擎、关键词和终端类型的 PV 。
- **网站分析**。各个页面的访问情况，包括及时了解哪些页面最吸引访客以及哪些页面最容易导致访客流失，从而帮助用户更有针对性地改善网站质量。常见指标是各页面的 PV。

###日志采集系统实现
我们简单模拟一下数据收集和发送的环节，用一个 Python 脚本随机生成 Nginx 访问日志，并通过脚本的方式自动上传至 HDFS ，然后移动至指定目录。 Spark Streaming 程序监控 HDFS 目录，自动处理新的文件。

# 实验步骤
### 0. 启动Hadoop
打开终端，输入：
```
service ssh restart
cd /usr/local/hadoop
sbin/start-dfs.sh
```

输入jps，确认NameNode以及DataNode已启动。

### 1. 编写生成日志的 Python 代码
使用文本编辑器创建文件sample\_web\_log.py。该Python脚本用于随机生成日志。

代码如下所示：
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import random
import time


class WebLogGeneration(object):

    # 类属性，由所有类的对象共享
    site_url_base = "http://www.xxx.com/"

    # 基本构造函数
    def __init__(self):
        #  前面7条是IE,所以大概浏览器类型70%为IE ，接入类型上，20%为移动设备，分别是7和8条,5% 为空
        #  https://github.com/mssola/user_agent/blob/master/all_test.go
        self.user_agent_dist = {0.0:"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)",
                                0.1:"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)",
                                0.2:"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727)",
                                0.3:"Mozilla/4.0 (compatible; MSIE6.0; Windows NT 5.0; .NET CLR 1.1.4322)",
                                0.4:"Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko",
                                0.5:"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:41.0) Gecko/20100101 Firefox/41.0",
                                0.6:"Mozilla/4.0 (compatible; MSIE6.0; Windows NT 5.0; .NET CLR 1.1.4322)",
                                0.7:"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0_3 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11B511 Safari/9537.53",
                                0.8:"Mozilla/5.0 (Linux; Android 4.2.1; Galaxy Nexus Build/JOP40D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Mobile Safari/535.19",
                                0.9:"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36",
                                1:" ",}
        self.ip_slice_list = [10, 29, 30, 46, 55, 63]
        self.url_path_list = ["login.php","view.php","list.php","upload.php","admin/login.php","edit.php","index.html"]
        self.http_refer = [ "http://www.baidu.com/s?wd={query}","http://www.google.cn/search?q={query}","http://www.sogou.com/web?query={query}","http://one.cn.yahoo.com/s?p={query}","http://cn.bing.com/search?q={query}"]
        self.search_keyword = ["spark","hadoop","hive","spark mlib","spark sql"]


    def sample_ip(self):
        slice = random.sample(self.ip_slice_list, 4) #从ip_slice_list中随机获取4个元素，作为一个片断返回
        return  ".".join([str(item) for item in slice])  #  todo


    def sample_url(self):
        return  random.sample(self.url_path_list,1)[0]


    def sample_user_agent(self):
        dist_uppon = random.uniform(0, 1)
        return self.user_agent_dist[float('%0.1f' % dist_uppon)]


    # 主要搜索引擎referrer参数
    def sample_refer(self):
        if random.uniform(0, 1) > 0.2:  # 只有20% 流量有refer
            return "-"

        refer_str=random.sample(self.http_refer,1)
        query_str=random.sample(self.search_keyword,1)
        return refer_str[0].format(query=query_str[0])

    def sample_one_log(self,count = 3):
        time_str = time.strftime("%Y-%m-%d %H:%M:%S",time.localtime())
        while count >1:
            query_log = "{ip} - - [{local_time}] \"GET /{url} HTTP/1.1\" 200 0 \"{refer}\" \"{user_agent}\" \"-\"".format(ip=self.sample_ip(),local_time=time_str,url=self.sample_url(),refer=self.sample_refer(),user_agent=self.sample_user_agent())
            print query_log
            count = count -1

if __name__ == "__main__":
    web_log_gene = WebLogGeneration()

    #while True:
    #    time.sleep(random.uniform(0, 3))
    web_log_gene.sample_one_log(random.uniform(10, 100))
```

使用文本编辑器创建文件genLog.sh。该脚本会调用上述sample\_web\_log.py脚本，真正创建日志。将genLog.sh保存至sample\_web\_log.py同一目录。

代码如下所示：
```
#!/bin/bash

# HDFS命令
HDFS="hadoop fs"

# Streaming程序监听的目录，注意跟后面Streaming程序的配置要保持一致
streaming_dir="/spark/streaming"

#创建目录
$HDFS -mkdir -p ${streaming_dir}/tmp

# 清空旧数据
$HDFS -rm "${streaming_dir}"'/tmp/*' > /dev/null 2>&1
$HDFS -rm "${streaming_dir}"'/*'     > /dev/null 2>&1

# 一直运行
while [ 1 ]; do
    ./sample_web_log.py > test.log

    # 给日志文件加上时间戳，避免重名
    tmplog="access.`date +'%s'`.log"

    # 先放在临时目录，再move至Streaming程序监控的目录下，确保原子性
    # 临时目录用的是监控目录的子目录，因为子目录不会被监控
    $HDFS -put test.log ${streaming_dir}/tmp/$tmplog
    $HDFS -mv           ${streaming_dir}/tmp/$tmplog ${streaming_dir}/
    echo "`date +"%F %T"` put $tmplog to HDFS succeed"
    sleep 1
done
```

### 2. 在 Spark Streaming 中编写Java代码
1. 打开Eclipse，新建Java项目，项目名为LogAnalyzer，包名为com.example，新建Java类，类名为LogAnalyzer。

2. 导入Spark相关Jar包

在项目名上点击右键，依次点击 Build Path >> Configure BuildPath >> Libraries  >> Add External JARs

在弹出的对话框中进入Spark jars目录 /usr/local/spark/jars，Ctrl+A选择全部的jar文件，点击OK。

3. 输入代码

```
package com.example;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;

import java.util.HashMap;
import java.util.Map;

public class LogAnalyzer {

    private static Map<String, String> searchEngine = new HashMap<>();

    static {
        searchEngine.put("www.google.cn", "q");
        searchEngine.put("www.yahoo.com", "p");
        searchEngine.put("cn.bing.com", "q");
        searchEngine.put("www.baidu.com", "wd");
        searchEngine.put("www.sogou.com", "query");
    }

    public static void main(String[] args) throws InterruptedException {
        SparkConf conf = new SparkConf().setAppName("LogAnalyzer").setMaster("local[2]");

        // 这是bin/spark-shell非交互式模式下创建StreamingContext的方法
        // 设计计算的周期，单位秒
        JavaStreamingContext ssc = new JavaStreamingContext(conf, Durations.seconds(30));

        /*
         * 创建输入DStream，是文本文件目录类型
         * 本地模式下也可以使用本地文件系统的目录，比如 file:///home/spark/streaming
         */
        JavaDStream<String> lines = ssc.textFileStream("hdfs://localhost:9000/spark/streaming");

        /*
         * 下面是统计各项指标，调试时可以只进行部分统计，方便观察结果
         */


        // 1. 总PV
        lines.count().print();

        // 2. 各IP的PV，按PV倒序
        // 空格分隔的第一个字段就是IP
        lines.mapToPair(new PairFunction<String, String, Integer>() {
            public Tuple2<String, Integer> call(String line) {
                String ip = line.split(" ")[0];
                return new Tuple2<>(ip, 1);
            }
        }).reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer op1, Integer op2) {
                return op1 + op2;
            }
        }).transformToPair(new Function<JavaPairRDD<String, Integer>, JavaPairRDD<String, Integer>>() {
            @Override
            public JavaPairRDD<String, Integer> call(JavaPairRDD<String, Integer> rdd) {
                return rdd.mapToPair(new PairFunction<Tuple2<String,Integer>, Integer, String>() {
                    //将键值对换，按值排序
                    @Override
                    public Tuple2<Integer, String> call(Tuple2<String, Integer> keyValueRDD) {
                        return keyValueRDD.swap();
                    }
                }).sortByKey(false)
                        .mapToPair(new PairFunction<Tuple2<Integer, String>, String, Integer>() {
                            // 将键值换回，便于输出
                            @Override
                            public Tuple2<String, Integer> call(Tuple2<Integer, String> valueKeyRDD) {
                                return valueKeyRDD.swap();
                            }
                        });
            }
        }).print();

        // 3. 搜索引擎PV
        JavaDStream<String> refer = lines.map(new Function<String, String>() {
            @Override
            public String call(String s) {
                return s.split("\"")[3];
            }
        });

        // 先输出搜索引擎和查询关键词，避免统计搜索关键词时重复计算
        // 输出(host, query_keys)
        JavaPairDStream<String, String> searchEnginInfo = refer.mapToPair(new PairFunction<String, String, String>() {
            @Override
            public Tuple2<String, String> call(String refer) {
                String[] refers = refer.split("/");
                String host = "";

                if(refers.length > 2){
                    host = refers[2];

                    if(searchEngine.containsKey(host)){
                        String queryString = refer.split("\\?")[1];
                        if(queryString.length() > 0){
                            String[] querys = queryString.split("&");
                            for (String query : querys){
                                if (query.startsWith(searchEngine.get(host) + "=")){
                                    return new Tuple2<>(host, query.split("=")[1]);
                                }
                            }
                        }
                    }
                }

                return new Tuple2<>(host, "");
            }
        });

        // 输出搜索引擎PV
        searchEnginInfo.filter(new Function<Tuple2<String, String>, Boolean>() {
            @Override
            public Boolean call(Tuple2<String, String> searchEngine) {
                return searchEngine._1.length() > 0;
            }
        }).mapToPair(new PairFunction<Tuple2<String,String>, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Tuple2<String, String> searchEngine) {
                return new Tuple2<>(searchEngine._1, 1);
            }
        }).reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer op1, Integer op2) {
                return op1 + op2;
            }
        }).print();

        // 4. 关键词PV
        searchEnginInfo.filter(new Function<Tuple2<String, String>, Boolean>() {
            @Override
            public Boolean call(Tuple2<String, String> searchEngine) {
                return searchEngine._2.length() > 0;
            }
        }).mapToPair(new PairFunction<Tuple2<String,String>, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Tuple2<String, String> searchEngine) {
                return new Tuple2<>(searchEngine._2, 1);
            }
        }).reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer op1, Integer op2) {
                return op1 + op2;
            }
        }).print();

        // 5. 终端类型PV
        lines.mapToPair(new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String line) {
                if(line.contains("iPhone")){
                    return new Tuple2<>("iPhone", 1);
                } else if(line.contains("Android")){
                    return new Tuple2<>("Android", 1);
                } else if(line.contains("Windows")){
                    return new Tuple2<>("Windows", 1);
                } else {
                    return new Tuple2<>("Other", 1);
                }
            }
        }).reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer op1, Integer op2) {
                return op1 + op2;
            }
        }).print();

        // 6. 各页面PV
        lines.mapToPair(new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String line) {
                return new Tuple2<>(
                        line.split("\"")[1].split(" ")[1],
                        1
                );
            }
        }).reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer op1, Integer op2) {
                return op1 + op2;
            }
        }).print();


        // 启动计算,等待执行结束（出错或Ctrl-C退出）
        ssc.start();
        ssc.awaitTermination();
    }
}

```

### 3. 启动实验，进行日志分析

1. 启动脚本，定位至genLog.sh所在目录；输入如下命令，随机生成日志。

```
chmod 700 genLog.sh sample_web_log.py
./genLog.sh
```

如看到类似put.XXXXXX.HDFS succeed提示则说明日志生成成功。

![](/pictures/1_20180417021751.051.png)

2. 在LogAnalyzer项目中，对文件LogAnalyzer.java右键->Run as...->Java Application.

可以在Console tab中找到如下统计指标：
- 总的PV

![](/pictures/2_20180417024829.029.png)

本次30秒窗口内共有159次访问记录。

- 各IP的PV

![](/pictures/3_20180416104927.027.png)

这里显示了排名前十的IP访问记录。

- 搜索引擎的PV

![](/pictures/4_20180417024840.040.png)

在我们的随机生成日志算法中，我们设置了20%的访问记录由搜索引擎引流的；本次30秒窗口内共有30次（7+5+9+3+6）访问记录是由搜索引擎引流而来的，占比18.9%（30/159），大致符合预期。
当然，如果记录时间越长，统计的访问记录次数越多，占比就能越接近20%。

- 关键词的PV

![](/pictures/5_20180416105142.042.png)

- 终端类型的PV

![](/pictures/6_20180417024846.046.png)

- 各页面的PV

![](/pictures/7_20180417024849.049.png)